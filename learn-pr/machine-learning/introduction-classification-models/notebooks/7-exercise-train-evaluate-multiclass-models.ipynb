{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Binary classification techniques work well when the data observations belong to one of two classes or categories, such as *true* or *false*. When the data can be categorized into more than two classes, you must use a *multiclass classification* algorithm.\n",
        "\n",
        "### Meet the data\n",
        "\n",
        "In this exercise, you'll build a multiclass classifier to separate penguins into categories of species.\n",
        "\n",
        "You'll be using *palmerpenguins* data, which contains size measurements for three penguin species that were observed on three islands in the Palmer Archipelago, Antarctica.\n",
        "\n",
        "> **Citation**: The *palmerpenguins* data was collected from 2007 to 2009 by Dr.Â Kristen Gorman with the [Palmer Station Long Term Ecological Research Program](https://pal.lternet.edu/), part of the [US Long Term Ecological Research Network](https://lternet.edu/). The data was imported directly from the [Environmental Data Initiative](https://environmentaldatainitiative.org/) (EDI) data portal, and it's available for use by CC0 license (\"No Rights Reserved\") in accordance with the [Palmer Station Data Policy](https://pal.lternet.edu/data).\n",
        "\n",
        "In R, the *palmerpenguins* package, by [Allison Marie Horst and Alison Presmanes Hill and Kristen B Gorman](https://allisonhorst.github.io/palmerpenguins/articles/intro.html), provides the data that's related to these adorable creatures.\n",
        "\n",
        "The CSV dataset that's used in the corresponding [Python learning module](https://learn.microsoft.com/en-us/learn/modules/train-evaluate-classification-models/) can be found in the [penguins.csv file](https://github.com/MicrosoftDocs/ml-basics/tree/master/data) on GitHub.\n",
        "\n",
        "Care for a pun? \n",
        "\n",
        "**Q**: What is a penguin's favorite movie?\n",
        "\n",
        "**A**: Frozen.\n",
        "\n",
        "With that, let's get started exploring some penguin data.\n",
        "\n",
        "### Explore the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the required packages and make them available in your current R session\n",
        "suppressPackageStartupMessages({\n",
        "  library(tidyverse)\n",
        "  library(palmerpenguins)\n",
        "  library(tidymodels)\n",
        "})\n",
        "\n",
        "# Take a peek into the data\n",
        "glimpse(penguins)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data contains the following columns:\n",
        "\n",
        "-   **species**: The name of the penguin species (Adelie, Chinstrap, Gentoo)\n",
        "\n",
        "-   **island**: The name of the island where the penguins were observed (in Palmer Archipelago, Antarctica)\n",
        "\n",
        "-   **bill_length_mm (also called culmen_length)**: The length of the dorsal ridge of the penguin's bill (millimeters)\n",
        "\n",
        "-   **bill_depth_mm (also called culmen_depth)**: The depth of the penguin's bill (millimeters)\n",
        "\n",
        "-   **flipper_length_mm**: The length of the penguin's flippers (millimeters)\n",
        "\n",
        "-   **body_mass_g**: The penguin's body mass (grams)\n",
        "\n",
        "-   **sex**: The penguin's sex (male, female)\n",
        "\n",
        "-   **year**: The study year (2007, 2008, or 2009)\n",
        "\n",
        "The **species** column, which contains penguin species Adelie, Chinstrap, or Gentoo, is the label that you want to train a model to predict.\n",
        "\n",
        "The corresponding [Python tutorial](https://learn.microsoft.com/en-us/learn/modules/train-evaluate-classification-models/7-exercise-multiclass-classification) uses a dataset with the following variables: *bill_length_mm*, *bill_depth_mm*, *flipper_length_mm*, *body_mass_g*, *species*.\n",
        "\n",
        "In this exercise, let's narrow the scope and use only those labels, and then make some summary statistics while we're at it. The [skimr package](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html) provides a strong set of summary statistics that are generated for a variety of data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the skimr column\n",
        "library(skimr)\n",
        "\n",
        "# Select desired columns\n",
        "penguins_select <- penguins %>% \n",
        "  select(c(bill_length_mm, bill_depth_mm, flipper_length_mm,\n",
        "           body_mass_g, species))\n",
        "\n",
        "# Do some summary statistics\n",
        "penguins_select %>% \n",
        "  skim()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the neat summary provided by skimr, you can see that each of the predictor columns has two missing values, and the label/outcome column has none.\n",
        "\n",
        "Now, dig a little deeper and filter the rows that contain missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins_select %>% \n",
        "  filter(if_any(everything(), is.na))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two of the rows contain no feature values at all (*NA* stands for *not available*). These rows won't be useful in training a model, so you can discard them from the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows containing missing values\n",
        "penguins_select <- penguins_select %>% \n",
        "  drop_na()\n",
        "\n",
        "# Confirm there are no missing values\n",
        "penguins_select %>% \n",
        "  anyNA()\n",
        "\n",
        "# Proportion of each species in the data\n",
        "penguins_select %>% \n",
        "  count(species)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you've dealt with the missing values, let's explore how the features relate to the label. \n",
        "\n",
        "Start by creating some box charts, as shown here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the paletteer package\n",
        "library(paletteer)\n",
        "\n",
        "# Pivot data to a long format\n",
        "penguins_select_long <- penguins_select %>% \n",
        "  pivot_longer(!species, names_to = \"predictors\", values_to = \"values\")\n",
        "\n",
        "# Make box plots\n",
        "theme_set(theme_light())\n",
        "penguins_select_long %>%\n",
        "  ggplot(mapping = aes(x = species, y = values, fill = predictors)) +\n",
        "  geom_boxplot() +\n",
        "  facet_wrap(~predictors, scales = \"free\") +\n",
        "  scale_fill_paletteer_d(\"nbapalettes::supersonics_holiday\") +\n",
        "  theme(legend.position = \"none\")\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the box plots, it appears that the Adelie and Chinstrap species have similar data profiles for bill_depth, flipper_length, and body_mass, but Chinstrap tends to have longer bill_length. Gentoo features tend to be fairly clearly differentiated from the others, which should help you train a good classification model.\n",
        "\n",
        "### Prepare the data\n",
        "\n",
        "Just as for binary classification, before you train the model, you need to split the data into subsets for training and validation. To maintain the proportion of each label value in the training and validation datasets, you'll also apply a *stratification* technique when you split the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Split data 70%-30% into training set and test set\n",
        "penguins_split <- penguins_select %>% \n",
        "  initial_split(prop = 0.70, strata = species)\n",
        "\n",
        "# Extract data in each split\n",
        "penguins_train <- training(penguins_split)\n",
        "penguins_test <- testing(penguins_split)\n",
        "\n",
        "# Print the number of observations in each split\n",
        "cat(\"Training cases: \", nrow(penguins_train), \"\\n\",\n",
        "    \"Test cases: \", nrow(penguins_test), sep = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and evaluate a multiclass classifier\n",
        "\n",
        "Now that you have a set of training features and corresponding training labels, you can fit a multiclass classification algorithm to the data to create a model.\n",
        "\n",
        "The function `parsnip::multinom_reg()` defines a model that uses linear predictors to predict multiclass data by using the multinomial distribution.\n",
        "\n",
        "Let's fit multinomial regression via the [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) package. This model usually has one tuning hyperparameter, `penalty`, which describes the amount of regularization. You use it to counteract any bias in the sample and help the model generalize well by avoiding *overfitting* the model to the training data. You can of course tune this parameter, as you'll do later in this exercise but, for now, let's choose an arbitrary value of *1*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify a multinomial regression via nnet\n",
        "multireg_spec <- multinom_reg(penalty = 1) %>% \n",
        "  set_engine(\"nnet\") %>% \n",
        "  set_mode(\"classification\")\n",
        "\n",
        "# Train a multinomial regression model without any preprocessing\n",
        "set.seed(2056)\n",
        "multireg_fit <- multireg_spec %>% \n",
        "  fit(species ~ ., data = penguins_train)\n",
        "\n",
        "# Print the model\n",
        "multireg_fit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can use the trained model to predict the labels for the test features and evaluate performance:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions for the test set\n",
        "penguins_results <- penguins_test %>% select(species) %>% \n",
        "  bind_cols(multireg_fit %>% \n",
        "              predict(new_data = penguins_test)) %>% \n",
        "  bind_cols(multireg_fit %>% \n",
        "              predict(new_data = penguins_test, type = \"prob\"))\n",
        "\n",
        "# Print predictions\n",
        "penguins_results %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the confusion matrix for our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "penguins_results %>% \n",
        "  conf_mat(species, .pred_class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The confusion matrix shows the intersection of predicted and actual label values for each class. In simpler terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
        "\n",
        "When you're dealing with multiple classes, it's ordinarily more intuitive to visualize this output as a heat map, like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "update_geom_defaults(geom = \"tile\", new = list(color = \"black\", alpha = 0.7))\n",
        "# Visualize confusion matrix\n",
        "penguins_results %>% \n",
        "  conf_mat(species, .pred_class) %>% \n",
        "  autoplot(type = \"heatmap\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The darker squares in the confusion matrix indicate high numbers of cases and, hopefully, you can see that they form a diagonal line that shows where the predicted and actual labels are the same.\n",
        "\n",
        "Calculate summary statistics for the confusion matrix, as shown here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summaries for the confusion matrix\n",
        "conf_mat(data = penguins_results, truth = species, estimate = .pred_class) %>% \n",
        "  summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tibble shows the overall metrics of how well the model performs across all three classes.\n",
        "\n",
        "Now let's evaluate the receiver operating characteristic (ROC) metrics. In the case of a multiclass classification model, a single ROC curve shows that a true positive rate versus false positive rate is not possible. However, you can use the rates for each class in a One-vs-Rest (OVR) comparison to create an ROC chart for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make an ROC_CURVE\n",
        "penguins_results %>% \n",
        "  roc_curve(species, c(.pred_Adelie, .pred_Chinstrap, .pred_Gentoo)) %>% \n",
        "  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n",
        "  geom_abline(lty = 2, color = \"gray80\", size = 0.9) +\n",
        "  geom_path(show.legend = T, alpha = 0.6, size = 1.2) +\n",
        "  coord_equal()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That went down well! The model did a great job in classifying the penguins.\n",
        "\n",
        "What kind of adventure would it be if you didn't preprocess the data for modeling?\n",
        "\n",
        "### Workflows with a support vector machine algorithm\n",
        "\n",
        "Again, just as with binary classification, you can use a workflow to apply preprocessing steps to the data before you fit it to an algorithm to train a model. Let's scale the numeric features in a transformation step before training. Just to show that you can, try a different algorithm (a *support vector machine* [SVM]) and tune some model hyperparameters.\n",
        "\n",
        "Support vector machine algorithms try to find a *hyperplane* in some feature space that best separates the classes. To learn more, see:\n",
        "\n",
        "- [Hands-on machine learning with R](https://bradleyboehmke.github.io/HOML/svm.html)\n",
        "- [An introduction to statistical learning with applications in R](https://www.statlearning.com/)\n",
        "- [Support vector machines under the hood](https://towardsdatascience.com/support-vector-machines-under-the-hood-c609e57a4b09)\n",
        "- [SVM kernels](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)\n",
        "\n",
        "You'll fit a *radial basis function* support vector machine to this data and tune the *SVM cost parameter* and the *Ï parameter* in the kernel function (the margin parameter doesn't apply to classification models).\n",
        "\n",
        "-   When you use a cost argument, you can specify the cost of a violation to the margin. When the cost argument is small, the margins are wide, and many support vectors will be on the margin or will violate the margin. This *could* make the model more robust and lead to better classification. When the cost argument is large, the margins will be narrow, and there will be few support vectors on the margin or that violate the margin.\n",
        "\n",
        "- As *Ï* decreases, the fit becomes more nonlinear, and the model becomes more flexible.\n",
        "\n",
        "Both parameters can have a profound effect on the model complexity and performance.\n",
        "\n",
        "The radial basis kernel is extremely flexible. For this reason, in practice, you would ordinarily start with this kernel when you're fitting SVMs.\n",
        "\n",
        "You mark parameters for tuning by assigning them a value of `tune()`.\n",
        "\n",
        "Also, how about trying out a new succinct way to create workflows that minimizes a lot of piping steps, as shown here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model specification\n",
        "svm_spec <- svm_rbf(mode = \"classification\", engine = \"kernlab\",\n",
        "            cost = tune(), rbf_sigma = tune())\n",
        "\n",
        "\n",
        "# Create a workflow that encapsulates a recipe and a model\n",
        "svm_wflow <- recipe(species ~ ., data = penguins_train) %>% \n",
        "  step_normalize(all_numeric_predictors()) %>% \n",
        "  workflow(svm_spec)\n",
        "\n",
        "# Print out workflow\n",
        "svm_wflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pretty neat, right?\n",
        "\n",
        "Now that you've specified what parameter to tune, you need to figure out a set of possible values to try out and then choose the best.\n",
        "\n",
        "To do this, you create a grid. In this example, you work through a regular grid of hyperparameter values, try them out, and see what pair results in the best model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Create a regular grid of six values for each tuning parameter\n",
        "svm_grid <- grid_regular(parameters(svm_spec), levels = 6)\n",
        "\n",
        "# Print out some parameters in your grid\n",
        "svm_grid %>% \n",
        "  slice_head(n = 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! One thing about hyperparameters is that they're not learned directly from the training set. Instead, you estimate them by using *simulated datasets* that are created from a process called *resampling*.\n",
        "\n",
        "Let's try out a *bootstrap resampling* technique. Bootstrap resampling means that you draw with a *replacement* from your original dataset, fit a model on that new set, which contains some duplicates, and then evaluate the model on the data points that weren't included.\n",
        "\n",
        "Then you repeat the process. The default behavior is 25 bootstraps, but this can be changed. \n",
        "\n",
        "Okay, let's create some simulated datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Bootstrap resampling\n",
        "penguins_bs <- bootstraps(penguins_train, times = 10)\n",
        "\n",
        "penguins_bs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tune your model via grid search\n",
        "\n",
        "You're ready to tune. Use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) to fit models at all the values you chose for each hyperparameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doParallel::registerDoParallel()\n",
        "\n",
        "# Model tuning via a grid search\n",
        "set.seed(2056)\n",
        "svm_res <- tune_grid(\n",
        "  object = svm_wflow,\n",
        "  resamples = penguins_bs,\n",
        "  grid = svm_grid\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have your tuning results, you can extract the performance metrics by using `collect_metrics()`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtain performance metrics\n",
        "svm_res %>% \n",
        "  collect_metrics() %>% \n",
        "  slice_head(n = 7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see if you can get more by visualizing the results:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the tuning metrics\n",
        "svm_res %>% \n",
        "  collect_metrics() %>% \n",
        "  mutate(rbf_sigma = factor(rbf_sigma)) %>% \n",
        "  ggplot(mapping = aes(x = cost, y = mean, color = rbf_sigma)) +\n",
        "  geom_line(size = 1.5, alpha = 0.7) +\n",
        "  geom_point(size = 2) +\n",
        "  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n",
        "  scale_x_log10(labels = scales::label_number()) +\n",
        "  scale_color_viridis_d(option = \"viridis\", begin = .1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems that an SVM with an *rbf_sigma* of 1 and 0.01 performed really well across all candidate values of cost. The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function can help you make a clearer distinction:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show best submodel \n",
        "svm_res %>% \n",
        "  show_best(\"accuracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Much better! Now, use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values in the best submodel:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model hyperparameters\n",
        "best_svm <- svm_res %>% \n",
        "  select_best(\"accuracy\")\n",
        "\n",
        "best_svm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect! These are the values for *rbf_sigma* and *cost* that maximize the accuracy of your penguin classification exercise.\n",
        "\n",
        "You can now finalize your workflow, so that the parameters you had marked for tuning by assigning them a value of `tune()` can get updated with the values from `best_svm`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalize the workflow\n",
        "final_wflow <- svm_wflow %>% \n",
        "  finalize_workflow(best_svm)\n",
        "\n",
        "final_wflow\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That marks the end of tuning.\n",
        "\n",
        "### The last fit\n",
        "\n",
        "Finally, by using your test data, fit this final model to the training data and evaluate how it would perform on new data. You can use the function [`last_fit()`](https://tune.tidymodels.org/reference/last_fit.html) with your finalized model. The function *fits* the finalized model on the full training dataset and *evaluates* it on the testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The last fit\n",
        "final_fit <- last_fit(object = final_wflow, split = penguins_split)\n",
        "\n",
        "# Collect metrics\n",
        "final_fit %>% \n",
        "  collect_metrics()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Much better! You can go ahead and obtain the hard class and probability predictions by using `collect_predictions()`, and you'll be well on your way to computing the confusion matrix and other summaries that come with it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect predictions and make a confusion matrix\n",
        "final_fit %>% \n",
        "  collect_predictions() %>% \n",
        "  conf_mat(truth = species, estimate = .pred_class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the model with new data observations\n",
        "\n",
        "Now, save your trained model so that you can use it again later. Begin by extracting the *trained workflow* object from the `final_fit` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the trained workflow\n",
        "penguins_svm_model <- final_fit %>% \n",
        "  extract_workflow()\n",
        "\n",
        "# Save the workflow\n",
        "library(here)\n",
        "saveRDS(penguins_svm_model, \"penguins_svm_model.rds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so now you have a trained model. Use it to predict the class of a new penguin observation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_psvm_model <- readRDS(\"penguins_svm_model.rds\") \n",
        "\n",
        "# Create a new tibble of observations\n",
        "new_obs <- tibble(\n",
        "  bill_length_mm = c(49.5, 38.2),\n",
        "  bill_depth_mm = c(18.4, 20.1),\n",
        "  flipper_length_mm = c(195, 190),\n",
        "  body_mass_g = c(3600, 3900))\n",
        "\n",
        "# Make predictions\n",
        "new_results <- new_obs %>% \n",
        "  bind_cols(loaded_psvm_model %>% \n",
        "              predict(new_data = new_obs))\n",
        "\n",
        "# Show predictions\n",
        "new_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good job! You now have a working model.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Classification is one of the most common forms of machine learning. By following the basic principles we've discussed in this exercise, you should be able to train, evaluate, and tune classification models with tidymodels. It's worth spending some time investigating classification algorithms in more depth, and a good starting point is the [tidymodels documentation](https://www.tidymodels.org/).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
