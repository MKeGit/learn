
1. Why Evaluate? Evaluating Generative AI (GenAI) applications is crucial to ensure that the system behaves as expected. This involves checking the effectiveness of the Large Language Model (LLM) solution, identifying any biases or ethical concerns, ensuring user satisfaction, and evaluating the cost and overall functionality of the AI system1.

2. Components of an AI System: An AI system is composed of several components, including document embedding, generation-model, user query, and other elements like vector search system, user interface, and security/governance tooling. Each of these components needs to be evaluated individually to ensure the system's overall performance2.

3. Evaluating Data: Evaluating data is a challenging task. It involves ensuring the quality of contextual data, monitoring changes, and reviewing for biases or unethical information. It's also crucial to confirm the legality of the data used and consult with legal teams for licensing requirements3.

4. Addressing Issues: Issues like data legality, harmful user behavior, and bias/ethical use must be addressed. For instance, users can input prompts intended to override the systemâ€™s intended use, leading to harmful or incorrect responses. Implementing oversight and guardrails can help mitigate these risks4.

5. Systematic Approach: A systematic approach to GenAI evaluation involves mitigating data risks with data licensing, prompt safety, and guardrails. It also includes evaluating the quality of LLMs, securing the system, and ensuring comprehensive, component-based evaluation5.

