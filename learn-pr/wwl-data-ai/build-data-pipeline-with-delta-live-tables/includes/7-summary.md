Delta Live Tables (DLT) are a powerful framework provided by Databricks that simplifies the construction and management of reliable data pipelines for big data and machine learning applications. Utilizing DLT, developers can define data transformations declaratively in Python or SQL, which the system automatically orchestrates and manages.

In this module, you learned how to:

- Describe Delta Live Tables
- Ingest data into Delta Live Tables
- Use Data Pipelines for Real time Data Processing

Additional reading

- [Delta Live Tables](https://learn.microsoft.com/azure/databricks/delta-live-tables/)
- [Loading data with Delta Live Tables](https://docs.databricks.com/en/delta-live-tables/load.html)
- [How to accelerate your ETL pipelines from 18 hours to as fast as 5 minutes with Azure Databricks](https://www.databricks.com/blog/2020/08/18/how-to-accelerate-your-etl-pipelines-from-18-hours-to-as-fast-as-5-minutes-with-azure-databricks.html)
