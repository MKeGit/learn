**Large Language Models** (**LLMs**) are designed to understand and generate human language, and their effectiveness hinges on four key components: 

- **Tasks**: The diverse language-related functions these models can perform, such as text classification, translation, and dialogue generation.
- **Tokenizer**: Preprocess text by breaking it down into manageable units, allowing the model to handle language efficiently.
- **Model**: Typically based on transformer architecture, utilizes self-attention mechanisms to process text and generate contextually relevant responses.
- **Prompt**: The inputs provided to the model, guiding it to produce the desired output.

Together, these components enable LLMs to perform a wide array of language tasks with high accuracy and fluency.

## Understand the tasks LLMs perform

LLMs are designed to perform a wide range of language-related tasks. LLMs are ideal for **natural language processing**, or **NLP** (1), tasks, because of their deep understanding of text and context.

:::image type="content" source="../media/natural-language-processing.png" alt-text="Screenshot of the model catalog in the Azure AI Studio.":::

One category of NLP tasks includes **natural language understanding**, or **NLU** (2), tasks such as sentiment analysis, named entity recognition (NER), and text classification, which involve extracting meaning and identifying specific elements within the text.

Another set of NLP tasks falls under **natural language generation**, or **NLG** (3), including text completion, summarization, translation, and content creation, where the model generates coherent and contextually appropriate text based on given inputs.

LLMs are also used in dialogue systems and **conversational** agents, where they can engage in human-like conversations, providing relevant and accurate responses to user queries.

## Understand the importance of the tokenizer

**Tokenization** is a vital preprocessing step in LLMs, where text is broken down into manageable units called **tokens**. These tokens can be words, subwords, or even individual characters, depending on the tokenization strategy employed.

Modern tokenizers, such as **Byte Pair Encoding** (**BPE**) and **WordPiece**, split rare or unknown words into subword units, allowing the model to handle out-of-vocabulary terms more effectively.

For example, consider the following sentence:

`I heard a dog bark loudly at a cat`

To tokenize this text, you can identify each discrete word and assign token IDs to them. For example:

```
- I (1)
- heard (2)
- a (3)
- dog (4)
- bark (5)
- loudly (6)
- at (7)
- a (3)
- cat (8)
```

The sentence can now be represented with the tokens:

`{1 2 3 4 5 6 7 3 8}`

Tokenization helps the model to maintain a balance between vocabulary size and representational efficiency, ensuring that it can process diverse text inputs accurately.

Tokenization also enables the model to convert text into numerical formats that can be efficiently processed during training and inference.

## Understand the underlying model architecture

The model architecture in LLMs is typically based on the **transformer** model, which utilizes **self-attention** mechanisms to process and understand text.

Transformers consist of layers of **encoders** and **decoders** that work together to analyze input text and generate outputs. The self-attention mechanism allows the model to *weigh the importance of different words* in a sentence, enabling it to capture long-range dependencies and context effectively.

:::image type="content" source="../media/transformer-model.png" alt-text="Diagram of transformer model architecture with the encoder and decoder blocks.":::

1. The **model** is trained on a large volume of natural language text.
2. The training data is broken down into **tokens** and the **encoder** block processes token sequences using **attention** to determine *relationships between tokens*.
3. The output from the encoder is a collection of **vectors** (multi-valued numeric arrays) in which each element of the vector represents a semantic attribute of the tokens. These vectors are referred to as **embeddings**.
4. The **decoder** block works on a new sequence of text tokens and uses the embeddings generated by the encoder to generate an appropriate natural language output.
5. For example, given an input sequence like `When my dog was` the model can use the attention mechanism to analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate completion of the sentence, such as `a puppy`.

This architecture is highly parallelizable, making it efficient for training on large datasets. The size of the model, often defined by the number of parameters, determines its capacity to store linguistic knowledge and perform complex tasks. Large models, such as GPT-3 and GPT-4, contain billions of parameters, which contribute to their high performance and versatility.

## Understand the importance of the prompt

**Prompts** are the initial inputs given to LLMs to guide their responses.

Crafting effective prompts is crucial for obtaining the desired output from the model. Prompts can range from simple instructions to complex queries, and the model generates text based on the context and information provided in the prompt.

For example, a prompt can be:

`Translate the following English text to French: "Hello, how are you?"`

The quality and clarity of the prompt significantly influence the model’s performance, as a well-structured prompt can lead to more accurate and relevant responses.

In addition to standard prompts, techniques such as **prompt engineering** involve refining and optimizing prompts to enhance the model’s output for specific tasks or applications.

An example of prompt engineering, where more detailed instructions are provided:

`Generate a creative story about a time-traveling scientist who discovers a new planet. Include elements of adventure and mystery.`

The interaction between tasks, tokenization, the model, and prompts is what makes LLMs so powerful and versatile. The model’s ability to perform various tasks is improved when you have effective tokenization, which ensures that text inputs are processed accurately. The transformer-based architecture allows the model to understand and generate text based on the tokens and the context provided by the prompts.
