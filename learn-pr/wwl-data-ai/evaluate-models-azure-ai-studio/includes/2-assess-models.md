When you

Evaluating your model's performance at different phases is crucial to ensure its effectiveness and reliability. Here are several approaches to consider.

## Model benchmarks

Comparing publicly available metrics across models and datasets is essential. These benchmarks help you understand how your model performs relative to others. Some commonly used benchmarks include:

- **Accuracy**: Compares model generated text with correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise.
- **Coherence**: Measures whether the model output flows smoothly, reads naturally, and resembles human-like language
- **Fluency**: Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses.
- **GPT Similarity**: Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.

## Manual evaluations

Manual evaluations involve human raters who assess the quality of the model's responses. This approach provides insights into aspects that automated metrics might miss, such as context relevance and user satisfaction. Human evaluators can rate responses based on criteria like relevance, informativeness, and engagement.

## Traditional machine learning metrics

Traditional machine learning metrics are also valuable in evaluating model performance. One such metric is the **F1-score**, which measures the ratio of the number of shared words between the generated and ground truth answers. The F1-score is useful for tasks like text classification and information retrieval, where precision and recall are important.

## AI-assisted metrics

AI-assisted metrics use advanced techniques to evaluate model performance. These metrics can include:

- **Risk and safety metrics**: These metrics assess the potential risks and safety concerns associated with the model's outputs. They help ensure that the model doesn't generate harmful or biased content.
- **Generation quality metrics**: These metrics evaluate the overall quality of the generated text, considering factors like creativity, coherence, and adherence to the desired style or tone.
