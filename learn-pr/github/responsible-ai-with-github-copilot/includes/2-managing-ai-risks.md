:::image type="content" source="../media/picture1.png" alt-text="Diagram showing a set of scales, representing the balance between the opportunities and risks posed by AI." lightbox="../media/picture1.png" border="false":::

Artificial Intelligence (AI) presents numerous opportunities for innovation and efficiency, but it also comes with significant risks that need to be carefully managed.

One of the primary concerns is that AI systems can sometimes make decisions that are difficult to interpret, leading to a lack of transparency and accountability. Additionally, AI can result in unintended and harmful outcomes, such as biased decision-making or privacy violations.

To mitigate these risks, it is essential to implement robust governance frameworks, ensure transparency in AI processes, and incorporate human oversight. By doing so, organizations can harness the benefits of AI while minimizing potential negative impacts.

In the next section, we’ll discuss the concept of responsible AI and how to apply its principles to reduce the risks associated with AI tools like GitHub Copilot.

## What is Responsible AI?

Responsible AI is an approach to developing, assessing, and deploying artificial intelligent systems in a safe, trustworthy, and ethical way. AI systems are the product of many decisions made by the people who develop and deploy them. From system purpose to how people interact with AI systems, Responsible AI can help proactively guide these decisions toward more beneficial and equitable outcomes. That means keeping people and their goals at the center of system design decisions and respecting enduring values like fairness, reliability, and transparency.

In the next unit, we’ll cover Microsoft and GitHub’s Six Principles of Responsible AI.
