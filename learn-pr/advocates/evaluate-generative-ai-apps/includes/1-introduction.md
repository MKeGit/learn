The generative AI development lifecycle is an iterative process used by Microsoft’s engineering teams to develop generative AI products and features. While each step in the process is essential to building trustworthy generative AI solutions, the measurement phase is critical to iterative development and getting apps into production. Azure AI provides a robust toolkit for evaluating generative AI in a repeatable, transparent way. This module introduces the key concepts of measuring the frequency and severity of various risks in AI systems.

## Learning objectives

By completing this module, you're able to:

- Apply best practices for choosing evaluation data
- Understood the purpose of and types of synthetic data for evaluation
- Comprehend the scope of the built-in metrics
- Choose the appropriate metrics based on your AI system use case
- Understand how to interpret evaluation results

## Prerequisites

- An Azure subscription – [Create one for free](https://azure.microsoft.com/free/cognitive-services/)
- Familiarity with Azure and the Azure portal