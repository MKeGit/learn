In this workshop, you learned how to utilize the Azure AI Content Safety features within the Content Safety Studio. You’re now ready to integrate this resource into Contoso Camping Store’s AI-powered platform!

As you continue in your journey in developing AI responsibly, remember to keep responsible AI principles at the forefront of your approach to integrating AI. Mitigating potential harms presented by generative AI models requires an iterative, layered approach that includes experimentation and measurement. Azure AI integrates years of AI policy, research, and engineering expertise from Microsoft so your teams can build safe, secure, and reliable AI solutions from the start, and leverage enterprise controls for data privacy, compliance, and security on infrastructure that is built for AI at scale.

> [!NOTE]
> After completing the workshop, if you've finished exploring Azure AI Services, delete the Azure resources that you created during the workshop.

### Learn more

- [Infuse responsible AI tools and practices in your LLMops](https://azure.microsoft.com/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/)
- [Harm categories in Azure AI Content Safety](/azure/ai-services/content-safety/concepts/harm-categories?tabs=warning)
- [Prompt Shields](/azure/ai-services/content-safety/concepts/jailbreak-detection)
- [Groundedness detection](/azure/ai-services/content-safety/concepts/groundedness)
