Risk & safety metrics are essential as they measure the tendency of AI systems to generate harmful, unsafe, or otherwise undesirable outputs. These metrics help in identifying potential risks and formulating mitigation strategies to address them. The key reasons these metrics matter include:

- **Measuring Harm**: Identifying and quantifying harmful content generated by AI systems to help ensure they don't spread misinformation, hate speech, or other malicious content.
- **Influencing Mitigation Strategy**: Providing actionable insights that can help in developing plans to mitigate identified risks and improve the overall safety of AI applications.
- **Building Trust**: Enhancing user trust by demonstrating a commitment to safety and responsibility in AI deployments.

## Scenario

Contoso Tales is developing an app that generates creative, age-appropriate campfire stories for children, tailored to their reading levels. The app builds on the story plot with input from the reader to craft unique, imaginative stories.

## Instructions

In this exercise, you assess an AI-generated excerpt for harm using risk and safety metrics. Open the `evaluate-risk-safety.ipynb` file to get started.

## Metrics

Provided is a table of our built-in risk and safety metrics:

|Metric  |What it Measures  |Assessment Output  |
|---------|---------|---------|
|Hateful and Unfair Content Defect Rate     | Measures the frequency of AI-generated content that contains hate speech or unfair biases.        |Severity level: 0 – 7, <brSeverity label: Very low, Low, Medium, and High|
|Sexual Content Defect Rate     |Measures the occurrence of AI-generated content containing inappropriate sexual references.         |Severity level: 0 – 7, <br> Severity label: Very low, Low, Medium, and High|
|Violent Content Defect Rate    |Evaluates the frequency of content that depicts or incites violence.         |Severity level: 0 – 7, <br> Severity label: Very low, Low, Medium, and High|
|Self-Harm Related Content Defect Rate     |Measures the generation of content that encourages or glamorizes self-harm.         | Severity level: 0 – 7, <br> Severity label: Very low, Low, Medium, and High|
|Jailbreak Defect Rate    |Measures how often AI systems can be manipulated to bypass safety protocols and restrictions. A jailbreak occurs when a user finds a way to get the AI to produce content the AI is designed to prevent.         | True or False        |
|Indirect Attack Rate     |Measures the susceptibility of AI to indirect prompt injections, where seemingly harmless prompts elicit inappropriate responses. An indirect prompt injection occurs when the AI is tricked into generating harmful content through a series of seemingly innocuous prompts.        | True or False        |
|Protected Material Defect Rate     |Evaluates how often AI systems generate content that infringes upon protected material, such as copyrighted text.         |True or False         |
