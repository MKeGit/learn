This module has provided an initial exploration of AI security testing, from understanding the unique challenges of AI systems to planning and executing effective red teaming exercises. As AI continues to evolve, so too must our approaches to security and safety, making AI red team practices essential for maintaining the integrity and trustworthiness of AI solutions.

## Further reading

The following articles and video presentations provide more information about Microsoft's approach to AI red teaming.

[Azure OpenAI Service - Azure OpenAI](/azure/ai-services/openai/concepts/prompt-engineering)

[Azure OpenAI Service content filtering - Azure OpenAI](/azure/ai-services/openai/concepts/content-filter)

[How Microsoft Approaches AI Red Teaming](https://www.youtube.com/watch?v=zFRn_RMSPI4)

[Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?msockid=050b46b3cf5664822aca5257ce6465b2)

[Red teams think like hackers to help keep AI safe](https://news.microsoft.com/source/features/ai/red-teams-think-like-hackers-to-help-keep-ai-safe/)
